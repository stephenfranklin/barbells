---
title: "Quantifying Effective Barbell Use"
author: "Stephen Franklin"
date: "July 17, 2014"
output: html_document
---

In this analysis, we predict whether the subject performed an exercise in one of five manners: correctly, or having made one of four common mistakes in movement. We found that the tree classification algorithm poorly predicted the class of movement (not better than chance), but that the random forest algorithm predicted it very well (OOB error of 0.6%). The prediction indicates that as a body performs an exercise, a mistake could easily be detected by monitoring whether certain movements excede defined boundaries (e.g. the waist extending forward or backward too much). Thus inertial  sensors could aid in improving the exercise quality.

Excerpt from [Groupware@LES](http://groupware.les.inf.puc-rio.br/har):    

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

For data recording we used four 9 degrees of freedom Razor
inertial measurement units (IMU), which provide three-axes
acceleration, gyroscope and magnetometer data at a joint
sampling rate of 45 Hz. 

The four IMUs are located on the dumbell, forearm, upper arm, and the waist.

#### Preliminary Examination of Dataset:

There are six subjects and 19,622 observations. Each subject has many observations over a short period of time (0.5 - 2.0 seconds).

There are occasional observatons for which the 'new_window' variable is marked 'yes', and which have additional features. There are features for each dimension and location: kurtosis, skewness, max, min, amplitude, avg, stddev, var; as well as 'var_total_accel_' for each location.

We are provided the test set, 'pml-testing.csv', so we ought to ascertain some important things about it. (Although one doesn't usually look at their test set, they do usually know some basic things about it.) Firstly, the test set is not time sliced, so we needn't worry about deriving time sliced predictions. Second, the test set doesn't contain any data for the additional features that accompany 'new_window = yes', therefore all of that data may be safely ignored. This test set is actually the grading test set and doesn't contain the outcomes. (The last column is 'problem_id' instead of 'classe'.) We'll make a new test set from 'pml-training.csv' to evaluate our predictions.

## Analysis

### Get the data
The data used in this analysis was collected by [Groupware@LES](http://groupware.les.inf.puc-rio.br/har).
```{r setup, echo=FALSE}
knitr::opts_chunk$set(cache=2)
```

```{r getdata, eval=FALSE}
data_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
data_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url = data_train, destfile = "pml-training.csv",method = "wget")
download.file(url = data_test, destfile = "pml-testing.csv",method = "wget")
```

### Load the data:
```{r load}
pml.training <- read.csv("pml-training.csv")
pml.testing <- read.csv("pml-testing.csv")
#edit(pml.training)
#edit(pml.testing)

### Cull unnecessary variables ###
except.these <- grep("kurtosis|skewness|max|min|amplitude|avg|stddev|var", names(pml.training))
except.these <- c(except.these,c(1:7)) ## time and identification variables (X, user_name, cvtd_timestamp, raw_timestamp_part_2, new_window, num_window).
pml.training1 <- pml.training[,-except.these]
#View(pml.training1)
sum(is.na(pml.training1))  ## Check for NAs. 0 is nice.
pml.testing1 <- pml.testing[,-except.these]

### Integer variables to numeric ###
which( colnames(pml.training1)=="classe" )  ## <-- classe is the outcome.
pml.training1[, -53] <- as.data.frame(lapply(pml.training1[,-53],as.numeric))
pml.testing1[, -53] <- as.data.frame(lapply(pml.testing1[,-53],as.numeric))
#str(pml.training1)
```

### R Libraries
```{r libraries, cache=FALSE, message=FALSE}
library(caret); library(randomForest); library(rpart)
```

### Split Data
We'll reserve 80% of our data for training and 20% for the test set.
```{r split}
set.seed(42)
inTrain <- createDataPartition(y=pml.training1$classe,
                              p=0.80, list=FALSE)
training <- pml.training1[inTrain,]
testing <- pml.training1[-inTrain,]
training[1,53]
class(training$classe)
dim(training);dim(testing)
```
The training set is now randomized (though equally distributed amongst the five classes of our outcome variable 'classe'.)

### Model with Trees
We'll train a model the Classification and Regression Tree (CART) algorithm (`method='rpart'`). We'll also use a 10-fold cross-validation (from within the `trControl` settings).

```{r tree, dependson='split'}
set.seed(42)
system.time( barb.tr <- train(classe ~ .,method="rpart",data=training, trControl = trainControl(method = "cv", number = 10)) )
####   user  system elapsed 
#### 36.995   0.913  41.745 
```

#### Trees Analysis
Our tree model is not better than chance. The in-sample error rate (from a 10-fold cross-validation) is 49.8% (`barb.tr$results`). The out-of-sample error rate is 50.2%.

```{r anal.tree, dependson='tree'}
barb.tr$finalModel

barb.tr$resample ## error rate of each fold
barb.tr$results ## cp = complexity parameter

confusionMatrix(barb.tr)

### Test the Tree Model ###
pred.te<- predict(barb.tr,newdata=testing)
testing$predtrRight <- pred.te==testing$classe
table(pred.te,testing$classe)
1-sum(testing$predtrRight)/nrow(testing)  ## out of sample error rate
```

### Model with Random Forest

#### This Random Forest took too long to calculate proximities with the 80% training set and with `prox=TRUE`.
```{r rf-fail, eval=FALSE, echo=TRUE}
### Train with 80% ###
set.seed(42)
system.time( barb80.rf <- train(classe~ .,data=training,method="rf",prox=TRUE) )
#### With 80% of the data in the training set,
#### this command took . . . failed. 
#### Timing stopped at: 33087.95 353.427 38917.49 
```

#### This Random Forest model uses a training set of 20% to keep the processing time down (`prox = TRUE`).

##### Split the Data (20/80)
```{r split20}
set.seed(42)
inTrain <- createDataPartition(y=pml.training1$classe,
                              p=0.20, list=FALSE)
training20 <- pml.training1[inTrain,]
testing80 <- pml.training1[-inTrain,]
training20[1,53]
class(training20$classe)
dim(training20);dim(testing80)
```

##### Train the Random Forest model (20%)
 
```{r rf20, dependson='split20'}
set.seed(42)
system.time( barb.rf <- train(classe ~., data=training20,method="rf",prox=TRUE) )
####     user   system  elapsed 
#### 3395.666   34.611 3509.028 
```

```{r rf20.model, dependson='rf20'}
barb.rf$finalModel
varImpPlot(barb.rf$finalModel)

### Get features in order of importance ###
imp <- as.data.frame(barb.rf$finalModel$importance)
imp <- cbind(feature = rownames(imp), imp)
rownames(imp)<-NULL
head(imp[with(imp, order(-MeanDecreaseGini)), ],10)
```
The out-of-bag error rate is 2.39%.

#### Predicting the random forest model with the test set of 80% 
The out-of-sample error rate is 2.46%
```{r anal.rf20, dependson='rf20'}
pred <- predict(barb.rf,testing80)
testing80$predRight <- pred==testing80$classe
table(pred,testing80$classe)

### Out-of-Sample error rate ###
1-sum(testing80$predRight)/nrow(testing80)
```

#### This Random Forest model uses an 80% Training set but doesn't calculate proximities (`prox=FALSE`).
```{r rf80, dependson='split'}
set.seed(42)
system.time( barb80.rf <- train(classe~ .,data=training,method="rf",prox=FALSE) )
####     user   system  elapsed 
#### 8358.456   66.568 8647.038 
```

```{r, rf80imp, dependson='rf80'}
### Get features in order of importance ###
imp80 <- as.data.frame(barb80.rf$finalModel$importance)
imp80 <- cbind(feature = rownames(imp80), imp80)
rownames(imp80)<-NULL
head(imp80[with(imp80, order(-MeanDecreaseGini)), ],10)

varImpPlot(barb80.rf$finalModel)

barb80.rf$finalModel
```

#### PREDICTION with the test set (20%)

The features' importance branches differently than the previous model. It attains an out-of-bag error rate of 0.5% and an out-of-sample error rate of 0.6%.
```{r anal.rf80, dependson='rf80'}
pred <- predict(barb80.rf,testing)
testing$predRight <- pred==testing$classe
table(pred,testing$classe)

table(testing$predRight,testing$classe)

### Out-of-Sample Error rate ###
1-sum(testing$predRight)/nrow(testing)
```


```{r, eval=FALSE, echo=FALSE}
### Submission script ###

answers <- as.character(pred)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)

plot(sort(modFit$importance),training) 


fancyRpartPlot(modFit)
summary(modFit)

```


```{r eval=FALSE, echo=FALSE}
### save object or environment to .Rdata ###
save.image()
```

#### Out-of-bag Error Estimate without Cross Validation
In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. Instead we use the out-of-bag error estimate.
http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr

---

## Works Cited

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 
Cited by 2 (Google Scholar)
