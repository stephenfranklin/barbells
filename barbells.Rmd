---
title: "Quantifying Effective Barbell Use"
author: "Stephen Franklin"
date: "July 17, 2014"
output: html_document
---

In this analysis, we predict whether the subject performed an exercise in one of five manners: correctly, or having made one of four common mistakes in movement.

Excerpt from [Groupware@LES](http://groupware.les.inf.puc-rio.br/har):    

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

For data recording we used four 9 degrees of freedom Razor
inertial measurement units (IMU), which provide three-axes
acceleration, gyroscope and magnetometer data at a joint
sampling rate of 45 Hz. 

The four IMUs are located on the dumbell, forearm, upper arm, and the waist.

#### Preliminary Examination of Dataset:

There are six subjects and 19,622 observations. Each subject has many observations over a short period of time (0.5 - 2.0 seconds).

There are occasional observatons for which the 'new_window' variable is marked 'yes', and which have additional features. There are features for each dimension and location: kurtosis, skewness, max, min, amplitude, avg, stddev, var; as well as 'var_total_accel_' for each location.

We are provided the test set, 'pml-testing.csv', so we ought to ascertain some important things about it. (Although one doesn't usually look at their test set, they do usually know some basic things about it.) Firstly, the test set is not time sliced, so we needn't worry about deriving time sliced predictions. Second, the test set doesn't contain any 'new_window = yes' data, therefore all of that data may be safely ignored. This test set is actually the grading test set and doesn't contain the outcomes (The last column is 'problem_id' instead of 'classe'. We'll make a new test set from 'pml-training.csv' to evaluate our predictions.

## Analysis

### Get the data
The data used in this analysis was collected by [Groupware@LES](http://groupware.les.inf.puc-rio.br/har).
```{r getdata, eval=FALSE}
data_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
data_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url = data_train, destfile = "pml-training.csv",method = "wget")
download.file(url = data_test, destfile = "pml-testing.csv",method = "wget")
```

### Load the data:
```{r load, cache=TRUE}
pml.training <- read.csv("pml-training.csv")
pml.testing <- read.csv("pml-testing.csv")
#edit(pml.training)
#edit(pml.testing)

### Cull unnecessary variables ###
except.these <- grep("kurtosis|skewness|max|min|amplitude|avg|stddev|var", names(pml.training))
except.these <- c(except.these,c(1:7)) ## time and identification variables (X, user_name, cvtd_timestamp, raw_timestamp_part_2, new_window, num_window).
pml.training1 <- pml.training[,-except.these]
#View(pml.training1)
sum(is.na(pml.training1))  ## Check for NAs. 0 is nice.
pml.testing1 <- pml.testing[,-except.these]

### Integer variables to numeric ###
which( colnames(pml.training1)=="classe" )
pml.training1[, -53] <- as.data.frame(lapply(pml.training1[,-53],as.numeric))
pml.testing1[, -53] <- as.data.frame(lapply(pml.testing1[,-53],as.numeric))
#str(pml.training1)
```

### R Libraries
```{r libraries, cache=FALSE}
library(caret); library(randomForest); library(rattle); library(rpart)
```

### Split Data
We'll reserve 20% of our data for the test set.
```{r split}
set.seed(42)
inTrain <- createDataPartition(y=pml.training1$classe,
                              p=0.20, list=FALSE)
training <- pml.training1[inTrain,]
testing <- pml.training1[-inTrain,]
training[1,56]
class(training$classe)
dim(training);dim(testing)
str(testing)
```
The training set is now randomized (though equally distributed amongst the five classes of our outcome variable 'classe'.)


### Model - Random Forests
```{r model, cache=TRUE}
set.seed(42)
#barb.rf <- randomForest(classe ~ ., data=training,prox=T)
#### barb.rf : 
#### With only 20% of the data in the training set,
#### this command took . . . 
####     user   system  elapsed 
#### 3531.559   27.930 3620.868
system.time( barb.rf <- train(classe~ .,data=training,method="rf",prox=TRUE) )
####     user   system  elapsed 
#### 3395.666   34.611 3509.028 
barb.rf$finalModel
varImpPlot(barb.rf$finalModel)
## looks like 'roll_belt' and 'raw_timestamp_part_1' are the two most important.
barb.c <- classCenter(training[,-53], training[,53], barb.rf$finalModel$prox)

fancyRpartPlot(barb.rf$finalModel)

sort(row.names(barb.rf$finalModel$importance), decreasing = T)

plot(training[,3], train[,4], pch=21, xlab=names(iris)[3], ylab=names(iris)[4],
bg=c("red", "blue", "green")[as.numeric(factor(iris$Species))],
main="Iris Data with Prototypes")
points(iris.p[,3], iris.p[,4], pch=21, cex=2, bg=c("red", "blue", "green"))
```

### Split Data AGAIN
We'll now reserve 80% of our data for the test set.
```{r split}
set.seed(42)
inTrain <- createDataPartition(y=pml.training1$classe,
                              p=0.80, list=FALSE)
training <- pml.training1[inTrain,]
testing <- pml.training1[-inTrain,]
training[1,56]
class(training$classe)
dim(training);dim(testing)
```


### Predicting with the test 80% test set
```{r}
pred <- predict(barb.rf,testing)
testing$predRight <- pred==testing$classe
table(pred,testing$classe)


plot(sort(modFit$importance),training) 
fancyRpartPlot(modFit)
summary(modFit)
```


### Let's try again with 80%:
```{r}
####  Now train with 80%
set.seed(42)
#### barb80.rf : 
#### With 80% of the data in the training set,
#### this command took . . . failed. 
#### Timing stopped at: 33087.95 353.427 38917.49 
#system.time( barb80.rf <- train(classe~ .,data=training,method="rf",prox=TRUE) )


#### Without prox,
####     user   system  elapsed 
#### 8080.899   62.442 8234.453 
system.time( barb80.rf <- train(classe~ .,data=training,method="rf",prox=FALSE) )
####     user   system  elapsed 
#### 8358.456   66.568 8647.038 

### PREDICTION with the test set (20%)
pred <- predict(barb80.rf,pml.testing)
testing$predRight <- pred==pml.testing$classe
table(pred,testing$classe)
barb80.rf$finalModel$importance



### submission
answers <- as.character(pred)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)

plot(sort(modFit$importance),training) 


fancyRpartPlot(modFit)
summary(modFit)

```


### train with trees (80/20)
```{r}
set.seed(42)
system.time( barb.tr <- train(classe ~ .,method="rpart",data=training) )
####    user  system elapsed 
#### 87.331   3.113  91.308
barb.tr$finalModel
plot(barb.tr$finalModel, uniform=TRUE, 
      main="Classification Tree")
text(barb.tr$finalModel, use.n=TRUE, all=TRUE, cex=.8)

fancyRpartPlot(barb.tr$finalModel)

barb.tr$results
barb.tr.imp<-sort(barb.tr$finalModel$variable.importance, decreasing = T)
barb.tr.imp

predict(barb.tr,newdata=testing)
```






## Out-of-bag Error Estimate without Cross Validation
In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error.
http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr






---

### Works Cited

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 
Cited by 2 (Google Scholar)

---

## Other notes, to be reviewed/deleted:
```r
dim(subset(training1, training1$classe=="A"))
[1] 5580   60
dim(subset(training1, training1$classe=="B"))
[1] 3797   60
dim(subset(training1, training1$classe=="C"))
[1] 3422   60
dim(subset(training1, training1$classe=="D"))
[1] 3216   60
dim(subset(training1, training1$classe=="E"))
[1] 3607   60
dim(subset(training1, training1$classe=="F"))
[1]  0 60

```
